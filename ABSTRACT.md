The authors present their collected and annotated **ArSL21L: Arabic Sign Language Letter Dataset** consisting of 14202 images of 32 letter signs with various backgrounds collected from 50 people. They benchmarked the ArSL21L dataset on state-of-the-art object detection models, i.e., 4 versions of YOLOv5.

## Motivation

For individuals with hearing loss (PwHL), establishing social connections with the mainstream population can be challenging. This often necessitates interactive computer systems capable of understanding sign language. With the growing popularity of Metaverse applications employing augmented reality (AR) and virtual reality (VR), there's an opportunity to facilitate remote sign language instruction through avatars that mimic gestures, powered by AI (Artificial Intelligence) systems. While numerous methods and datasets exist for English sign language, resources for Arabic sign language are limited. Thus, the authors introduce the Arabic Sign Language Letters Dataset (ArSL21L), comprising 14,202 images depicting 32 letter signs against various backgrounds. These images were collected from 50 individuals and annotated for clarity and consistency.

<img src="https://github.com/dataset-ninja/arsl21l/assets/120389559/aa16afc6-aa46-4f74-b4ba-f9a93c624190" alt="image" width="600">

<span style="font-size: smaller; font-style: italic;">Arabic sign language.</span>

